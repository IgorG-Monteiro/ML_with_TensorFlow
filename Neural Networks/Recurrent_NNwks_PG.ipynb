{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN PLAY GENERATOR**\n",
    "\n",
    "We are going to use a *RNN* to generate a play. We will show the *RNN* an example of something we want it to recreate and it will learn how to write a version on of it on its own. Based on: https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOWNLOADING THE DATASET\n",
    "\n",
    "# Loading romeo and juliet shakespeare play\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
    "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# OR IF I WANTED TO LOAD MY OWN DATA I CAN JUST (TXT FILE ONLY)\n",
    "\n",
    "# from google.colab import files\n",
    "# path_to_file = list(files.upload().keys())[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 1115394 characters\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# READ CONTENTS OF FILE\n",
    "\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8') # read and decode to py2 compat\n",
    "print('Text length: {} characters\\n'.format(len(text)))\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n",
      "Decoded: First Citizen\n"
     ]
    }
   ],
   "source": [
    "# ENCODING\n",
    "# we are going to encode each unique character as a different integer\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# creating the StringLookup layer\n",
    "ids_from_chars = tf.keras.layers.StringLookup( \n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "# same layer but to do the opposite\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "text_as_int = text_to_int(text)\n",
    "\n",
    "# DECODING\n",
    "# Function that do the opposite (numeric to text)\n",
    "def int_to_text(ints):\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))\n",
    "print(\"Decoded:\", int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING TRAINING EXAMPLES\n",
    "# we need to to split our data from above into many shorter sequences that we can pass to the model as training examples\n",
    "# will use a seq_length sequence as input and a seq_length sequence as the output, where the original one is shifted\n",
    "# one letter to the right as below\n",
    "''' INPUT: Hell || OUTPUT: ello '''\n",
    "\n",
    "seq_length = 100 \n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # create training examples/targets\n",
    "\n",
    "# Using the batch method to turn this stream of characters into batches of desired length\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "INPUT: First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT: irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "INPUT: are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT: re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "# Splitting those sequences into input and output\n",
    "\n",
    "def split_input_target(chunk): # Hello\n",
    "    input_text = chunk[:-1] # hell\n",
    "    target_text = chunk[1:] # ello\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target) # using MAP to apply the function to every entry\n",
    "\n",
    "# peeking at some examples:\n",
    "for x, y in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\nINPUT:\", int_to_text(x))\n",
    "    print(\"\\nOUTPUT:\", int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKING TRAINING BATCHES\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab) # number of unique characters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "BUFFER_SIZE = 10000 # Buffer size to shuffle the dataset \n",
    "\n",
    "data = ( # shuffling the data maintaining a buffer\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m5,246,976\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)         │        \u001b[38;5;34m66,625\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BUILDING THE MODEL\n",
    "# We will be using a embedding layer, a LSTM and one dense layer that contains a node for each unique character in train data.\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(batch_size=batch_size, \n",
    "                              shape=[None,]),\n",
    "        tf.keras.layers.Embedding(vocab_size, \n",
    "                                  embedding_dim),  \n",
    "        tf.keras.layers.LSTM(rnn_units, \n",
    "                             return_sequences=True, \n",
    "                             stateful=True, \n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING A LOSS FUNCTION**\n",
    "\n",
    "Actually creating our own loss function. Because our model will output a (64, sequence_length, 65) shaped tensor \n",
    "that represents the probability distribution of each character at each timestep for every sequence in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# looking  at a sample input and the output from our untrained model (to understand what the model is giving us)\n",
    "\n",
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    example_batch_predictions = model(input_example_batch) # ask our mopdel for a predition on our first batch of train data\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-1.5734300e-03  4.6204082e-03 -5.5786385e-03 ...  5.4804608e-03\n",
      "    2.1179349e-03  7.4942363e-04]\n",
      "  [-4.8483294e-03  7.3886430e-03 -4.3548988e-03 ...  4.9965410e-03\n",
      "   -1.1944374e-03  1.6559919e-03]\n",
      "  [-4.1226028e-03  6.4025582e-03 -4.4738958e-03 ...  1.1736347e-03\n",
      "    2.0886306e-03  3.8021035e-04]\n",
      "  ...\n",
      "  [-8.5923159e-03  5.6818016e-03 -4.0263501e-03 ... -1.0340010e-02\n",
      "   -1.2922564e-03  6.1157295e-03]\n",
      "  [-8.9632273e-03  5.0648283e-03 -4.2159297e-03 ... -9.7164242e-03\n",
      "    6.0256356e-03  4.2135273e-03]\n",
      "  [-9.0943985e-03  7.1077449e-03 -1.0176841e-02 ... -3.5476063e-03\n",
      "    5.9917886e-03  4.3523982e-03]]\n",
      "\n",
      " [[ 2.4283074e-03  3.1801707e-03  5.2154232e-03 ... -9.9292421e-04\n",
      "    2.4134885e-03  4.5613199e-03]\n",
      "  [-2.1494378e-03  7.0607150e-03  3.7700222e-03 ... -5.9401756e-04\n",
      "   -1.0570920e-03  3.8574662e-03]\n",
      "  [-3.0913348e-03  5.1345997e-03  7.4095314e-04 ... -2.3850887e-03\n",
      "   -1.4665499e-03  3.7637157e-03]\n",
      "  ...\n",
      "  [-8.2229115e-03  2.4397820e-03 -3.5996814e-03 ... -5.0724423e-03\n",
      "    1.8945718e-03  2.1503200e-03]\n",
      "  [-6.5915515e-03 -3.9334549e-04 -4.4721658e-03 ... -9.2052501e-03\n",
      "   -7.3682319e-04  7.1524484e-03]\n",
      "  [-7.6398370e-03  3.0133836e-03 -9.7186966e-03 ... -1.5615460e-03\n",
      "    1.1435804e-03  5.6879390e-03]]\n",
      "\n",
      " [[-4.5700595e-03  3.7808497e-03 -8.2224101e-04 ... -2.2920656e-03\n",
      "   -2.1815142e-03  3.7010224e-03]\n",
      "  [-3.6485135e-03  9.2109265e-03 -1.0931293e-03 ...  2.2898831e-03\n",
      "   -1.9041810e-03  4.9543614e-03]\n",
      "  [-3.7052189e-03  4.1832086e-03 -3.2285089e-03 ... -3.2242294e-04\n",
      "   -1.6578694e-03  3.6863112e-03]\n",
      "  ...\n",
      "  [-5.5746161e-03 -3.7898910e-03 -1.3539267e-02 ...  2.4915445e-03\n",
      "   -3.7757177e-03 -2.9606163e-04]\n",
      "  [-2.2438257e-03 -5.6847222e-03 -3.7964927e-03 ...  5.8829677e-03\n",
      "   -6.3199508e-03 -5.1005394e-03]\n",
      "  [-6.1183395e-03 -1.8278002e-03 -4.1176728e-03 ...  3.4862766e-03\n",
      "   -7.2815972e-03 -2.9240502e-05]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.0819341e-03 -6.9153022e-05 -4.9956515e-03 ...  3.3382101e-03\n",
      "   -7.5193029e-03 -3.4897027e-03]\n",
      "  [-3.3939380e-04 -2.4450740e-03 -5.4616691e-03 ...  1.6418826e-03\n",
      "   -6.8984320e-03 -2.4963703e-03]\n",
      "  [ 4.4321055e-03 -3.6867599e-03 -7.0125512e-03 ...  4.0490190e-03\n",
      "   -6.1355764e-03 -3.1521772e-03]\n",
      "  ...\n",
      "  [ 1.0205174e-03  5.7484508e-03  1.5471322e-03 ...  7.1502263e-03\n",
      "   -4.1356576e-03  3.3293318e-03]\n",
      "  [ 3.0410641e-03  3.8889654e-03 -4.5677940e-03 ...  9.2457142e-03\n",
      "   -1.1887849e-02 -5.4625981e-04]\n",
      "  [ 1.8036892e-03  3.7008661e-03 -5.3744004e-03 ...  9.2661055e-03\n",
      "   -9.6805049e-03 -2.1745472e-03]]\n",
      "\n",
      " [[-1.5734300e-03  4.6204082e-03 -5.5786385e-03 ...  5.4804608e-03\n",
      "    2.1179349e-03  7.4942363e-04]\n",
      "  [ 2.0048919e-03  5.3404598e-03 -3.2532217e-03 ...  4.4084331e-03\n",
      "    4.3454058e-03 -2.9138708e-03]\n",
      "  [ 6.0651964e-03  2.1480459e-03 -6.4259963e-03 ...  4.1818772e-03\n",
      "    1.8700734e-03 -4.1190940e-03]\n",
      "  ...\n",
      "  [-7.6993462e-03  7.4114241e-03 -4.4511054e-03 ...  1.6182819e-03\n",
      "   -9.6969577e-03  2.9943124e-03]\n",
      "  [-5.5388580e-03  5.4388708e-03 -8.6137950e-03 ...  8.9426907e-03\n",
      "   -9.1927070e-03  1.4172297e-03]\n",
      "  [-4.3374919e-03  4.7531803e-03 -9.7707380e-03 ...  9.0446156e-03\n",
      "   -7.3820841e-03 -1.4766005e-03]]\n",
      "\n",
      " [[-1.8766520e-03 -2.1482771e-03 -1.2266196e-03 ... -1.4913061e-03\n",
      "   -1.2607900e-03 -6.0543360e-04]\n",
      "  [-3.2701674e-03 -4.0394468e-03 -1.3459034e-03 ... -1.9727317e-03\n",
      "   -2.6444730e-03 -1.2103494e-03]\n",
      "  [-4.2256445e-04 -5.1484504e-03  6.5008597e-03 ...  1.7120334e-03\n",
      "   -5.0735208e-03 -5.8270656e-03]\n",
      "  ...\n",
      "  [-4.1907867e-03  3.1811965e-03 -4.1188644e-03 ... -1.8917170e-03\n",
      "   -6.3277775e-04  7.4481834e-03]\n",
      "  [-1.5195813e-03  3.8143310e-03 -2.3654411e-03 ... -2.3636001e-03\n",
      "    2.5762836e-03  1.5937665e-03]\n",
      "  [-3.9752870e-04  1.9604096e-03 -7.1654785e-03 ... -9.4626303e-05\n",
      "   -6.1192643e-03 -3.0631053e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# the prediction is an array of 64 arrays, one for each entry in the batch\n",
    "\n",
    "print(len(example_batch_predictions))\n",
    "print(example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "tf.Tensor(\n",
      "[[-0.00157343  0.00462041 -0.00557864 ...  0.00548046  0.00211793\n",
      "   0.00074942]\n",
      " [-0.00484833  0.00738864 -0.0043549  ...  0.00499654 -0.00119444\n",
      "   0.00165599]\n",
      " [-0.0041226   0.00640256 -0.0044739  ...  0.00117363  0.00208863\n",
      "   0.00038021]\n",
      " ...\n",
      " [-0.00859232  0.0056818  -0.00402635 ... -0.01034001 -0.00129226\n",
      "   0.00611573]\n",
      " [-0.00896323  0.00506483 -0.00421593 ... -0.00971642  0.00602564\n",
      "   0.00421353]\n",
      " [-0.0090944   0.00710774 -0.01017684 ... -0.00354761  0.00599179\n",
      "   0.0043524 ]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Examination of one prediction (2d array of length 100, where each interior array is the prediction for the next character)\n",
    "\n",
    "pred = example_batch_predictions[0]\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[-1.5734300e-03  4.6204082e-03 -5.5786385e-03  3.1328262e-03\n",
      "  2.5488259e-03  1.1373349e-03  2.1269997e-03  1.1709330e-03\n",
      " -1.4056347e-03  7.6413795e-04 -5.6327092e-03  3.5331040e-03\n",
      "  5.9535261e-05 -3.5696612e-03  3.3087162e-03  4.2239451e-03\n",
      " -5.8806292e-04  2.3046955e-03 -5.6508100e-03 -2.7600070e-03\n",
      " -1.0754797e-04 -5.7352521e-04  1.1849438e-03 -1.7078919e-04\n",
      "  6.2300009e-03 -4.8578568e-03 -1.9640685e-03 -3.8548068e-03\n",
      " -4.5219050e-03  8.1414748e-03 -6.5348169e-04 -1.5593697e-03\n",
      " -2.7652807e-04 -3.4174628e-03 -1.9910380e-03 -9.3363579e-03\n",
      "  5.8505400e-03 -4.1685752e-03  1.8875634e-03 -4.7506667e-03\n",
      "  5.7707522e-03  2.0410002e-03  4.5604544e-04  6.6733062e-03\n",
      " -2.4346036e-03  4.7878451e-03  4.9976055e-03 -9.3357614e-04\n",
      " -3.6273650e-03  4.7359955e-03 -1.1433380e-03 -5.1783142e-04\n",
      "  6.0219970e-03  4.6808347e-03 -2.4821749e-04 -1.9831012e-03\n",
      "  3.8046035e-04  8.2397519e-04  1.8877513e-04 -5.4651136e-03\n",
      " -2.6196926e-03  2.2336445e-03  5.4804608e-03  2.1179349e-03\n",
      "  7.4942363e-04], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# A prediction at the first timestep (65 values representing the probability of each character occurring next)\n",
    "\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wyZgzmwFhswCNySvGh.fLzk&\\nf$:SyJIjMwrTz$UJeIQBWRz,HV?.X,3ruRU:!i snQl.EJit,gTJsPKMhhuLsaXjjnQpqGUQzbg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample the output distribution (picking a value based on probability)\n",
    "\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# reshape that array and convert all the integers to numbers to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "predicted_chars # this is what the model predicts for sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION OF THE LOSS FUNCTION\n",
    "# The loss function needs to compare the output to the expected output and give a numeric value of how close the two were\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILING THE MODEL\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING CHECKPOINTS\n",
    "# allowing us to load our model from a checkpoint and continue training it\n",
    "\n",
    "checkpoint_dir = './RNN_PG_training_checkpoints' # directory will be saving it\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\") # file name\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 861ms/step - loss: 1.5590\n",
      "Epoch 2/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 909ms/step - loss: 1.4497\n",
      "Epoch 3/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 899ms/step - loss: 1.3822\n",
      "Epoch 4/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 908ms/step - loss: 1.3312\n",
      "Epoch 5/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 925ms/step - loss: 1.2936\n",
      "Epoch 6/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 935ms/step - loss: 1.2590\n",
      "Epoch 7/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m154s\u001b[0m 891ms/step - loss: 1.2248\n",
      "Epoch 8/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 907ms/step - loss: 1.1954\n",
      "Epoch 9/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 881ms/step - loss: 1.1627\n",
      "Epoch 10/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 909ms/step - loss: 1.1314\n",
      "Epoch 11/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 895ms/step - loss: 1.0977\n",
      "Epoch 12/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 853ms/step - loss: 1.0651\n",
      "Epoch 13/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 830ms/step - loss: 1.0292\n",
      "Epoch 14/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 828ms/step - loss: 0.9937\n",
      "Epoch 15/15\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 833ms/step - loss: 0.9597\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL\n",
    "\n",
    "history = model.fit(data, epochs=15, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE MODEL\n",
    "# Rebuilding the model from a checkpoint \n",
    "\n",
    "checkpoint_num = 15\n",
    "model.load_weights(\"./RNN_PG_training_checkpoints/ckpt_\" + str(checkpoint_num) + \".weights.h5\")\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, char2idx, idx2char):\n",
    "    num_generate = 800  # number of characters to generate\n",
    "    \n",
    "    # Vectorize input string\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []  # empty list to store results\n",
    "    \n",
    "    # Temperature parameter controls randomness\n",
    "    temperature = 1.0\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # Get the predictions for the last character in the sequence\n",
    "        predictions = predictions[0, -1, :]  # Shape [vocab_size]\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        predicted_id = tf.random.categorical(tf.expand_dims(predictions, 0), num_samples=1)[0, 0].numpy()\n",
    "        \n",
    "        # Add predicted character to generated text\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "        # Prepare the next input (just the predicted character)\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeous news:\n",
      "Throw died thy mother, wife, have warm or lie;\n",
      "And who possess'd them that with a guest\n",
      "Marr'd for his shame; and here I strive to-morrow,\n",
      "Or never yet this vount to high a dozen of the park,\n",
      "Have thought i' the might have given alightent,\n",
      "With that slept with thems! were I but think\n",
      "Incurted to his chiefest finger,\n",
      "Hoothing abuse much more, or gallows, dresses thee at hangs:\n",
      "For, being tricks now, the Duke of Norfolk, boy.\n",
      "\n",
      "MIRANDA:\n",
      "He; and, my lord, thy kinsman moved, had\n",
      "conscience to your great course to know a whoreison\n",
      "That would believe me: ther straight restrained so.\n",
      "\n",
      "Tribundance took you shield not her offences,\n",
      "A circumstance, is now home more heads your king.\n",
      "Rost know, gentle vanotock them down;\n",
      "Grace of my brafs can king, I'll wear thy strength,\n",
      "Yet by blood came fro\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp, char2idx, idx2char))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
